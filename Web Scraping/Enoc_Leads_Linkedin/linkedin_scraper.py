import asyncio
import csv
from datetime import datetime
from playwright.async_api import async_playwright
import random
import re
import pandas as pd
import os
import langid
import time
from functools import wraps
from typing import List, Dict, Optional

# ==================== CLASES DE APOYO SIMPLIFICADAS ====================

class ExecutionMetrics:
    """Sistema de m√©tricas simplificado"""
    
    def __init__(self):
        self.metrics = {
            'start_time': None,
            'end_time': None,
            'total_duration': None,
            'countries_processed': {},
            'terms_processed': {},
            'posts_found': 0,
            'posts_added': 0,
            'posts_filtered': 0,
            'errors_count': 0,
            'india_posts_filtered': 0  # Nueva m√©trica para posts de India filtrados
        }
    
    def start_timer(self):
        self.metrics['start_time'] = time.time()
        self._log("üöÄ Iniciando ejecuci√≥n del scraper")
    
    def end_timer(self):
        self.metrics['end_time'] = time.time()
        self.metrics['total_duration'] = self.metrics['end_time'] - self.metrics['start_time']
        self._log(f"‚è∞ Duraci√≥n total: {self.metrics['total_duration']:.2f} segundos")
    
    def increment(self, metric_name, value=1, details=None):
        if metric_name in self.metrics:
            if isinstance(self.metrics[metric_name], dict) and details:
                if details not in self.metrics[metric_name]:
                    self.metrics[metric_name][details] = 0
                self.metrics[metric_name][details] += value
            else:
                self.metrics[metric_name] += value
    
    def _log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}"
        print(log_message)
    
    def generate_report(self):
        report = [
            "\n" + "="*60,
            "üìä REPORTE DE EJECUCI√ìN DEL SCRAPER",
            "="*60,
            f"‚è∞ Duraci√≥n total: {self.metrics['total_duration']:.2f} segundos",
            f"üåé Pa√≠ses procesados: {len(self.metrics['countries_processed'])}",
            f"üîç T√©rminos procesados: {len(self.metrics['terms_processed'])}",
            f"üìÑ Publicaciones encontradas: {self.metrics['posts_found']}",
            f"‚úÖ Publicaciones a√±adidas: {self.metrics['posts_added']}",
            f"üö´ Publicaciones filtradas: {self.metrics['posts_filtered']}",
            f"üáÆüá≥ Publicaciones de India filtradas: {self.metrics['india_posts_filtered']}",
            f"‚ùå Errores: {self.metrics['errors_count']}",
            "="*60
        ]
        
        if self.metrics['countries_processed']:
            report.append("\nüìà Detalles por pa√≠s:")
            for country, count in self.metrics['countries_processed'].items():
                report.append(f"   {country}: {count} publicaciones")
        
        if self.metrics['terms_processed']:
            report.append("\nüìà Detalles por t√©rmino:")
            for term, count in self.metrics['terms_processed'].items():
                report.append(f"   '{term}': {count} publicaciones")
        
        report_text = "\n".join(report)
        self._log(report_text)
        return report_text

class ErrorHandler:
    """Manejador simplificado de errores"""
    
    @staticmethod
    def async_retry(max_retries=3, delay=2, backoff=2, exceptions=(Exception,)):
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                retries, current_delay = 0, delay
                while retries < max_retries:
                    try:
                        return await func(*args, **kwargs)
                    except exceptions as e:
                        retries += 1
                        if retries >= max_retries:
                            print(f"‚ùå Error despu√©s de {max_retries} intentos: {e}")
                            raise
                        
                        print(f"‚ö†Ô∏è Intento {retries}/{max_retries} fallido. Reintentando en {current_delay}s...")
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff
                return await func(*args, **kwargs)
            return wrapper
        return decorator

# ==================== CLASE PRINCIPAL MEJORADA ====================

class LinkedInBIMScraperFixed:
    def __init__(self):
        # T√âRMINOS DE B√öSQUEDA
        self.search_terms = [
            "BIM Modeler", "BIM Manager", "BIM Coordinator", "BIM Specialist",
            "BIM Designer", "BIM Engineer", "BIM Architect", "Revit Specialist",
            "Revit Modeler", "Revit Designer", "3D BIM", "VDC Engineer",
            "Building Information Modeling", "BIM Consultant", "BIM Project Manager"
        ]
        
        # PA√çSES
        self.countries = {
            "United States": {
                "code": "%5B%22urn%3Ali%3Afs_geo%3A103644278%22%5D",
                "location_names": ["USA", "EEUU", "United States", "Estados Unidos"]
            },
            "Canada": {
                "code": "%5B%22urn%3Ali%3Afs_geo%3A101174742%22%5D",
                "location_names": ["Canada", "Canad√°"]
            },
            "United Kingdom": {
                "code": "%5B%22urn%3Ali%3Afs_geo%3A101165590%22%5D",
                "location_names": ["UK", "United Kingdom", "Reino Unido"]
            },
            "Spain": {
                "code": "%5B%22urn%3Ali%3Afs_geo%3A105646813%22%5D",
                "location_names": ["Spain", "Espa√±a"]
            },
            "Colombia": {
                "code": "%5B%22urn%3Ali%3Afs_geo%3A100876405%22%5D",
                "location_names": ["Colombia", "CO", "Bogot√°", "Medell√≠n", "Cali"]
            }
        }
        
        self.results = []
        self.max_posts_per_term = 40  # L√≠mite m√°s conservador
        self.max_pages = 5  # P√°ginas m√°s conservadoras
        self.max_posts_total = 1000  # L√≠mite total m√°s realista
        
        self.consolidated_csv = "linkedin_bim_posts_consolidated.csv"
        self.consolidated_excel = "linkedin_bim_posts_consolidated.xlsx"
        
        # Componentes simplificados
        self.error_handler = ErrorHandler()
        self.metrics = ExecutionMetrics()

    def is_india(self, text):
        """Detectar si el texto contiene indicadores de la India"""
        if text == "N/A" or not text:
            return False
            
        india_indicators = [
            'india', 'indian', 'bangalore', 'mumbai', 'delhi', 'chennai', 
            'hyderabad', 'kolkata', 'pune', 'bengaluru', 'noida', 'gurgaon',
            'ahmedabad', 'india\'s', 'from india', 'based in india', 'located in india',
            'bombay', 'madras', 'calcutta', 'new delhi', 'india based', 'working in india',
            'üáÆüá≥', 'indianapolis'  # Indianapolis podr√≠a ser falso positivo, pero es poco com√∫n
        ]
        
        text_lower = text.lower()
        # Verificar m√∫ltiples indicadores para reducir falsos positivos
        india_count = sum(1 for indicator in india_indicators if indicator in text_lower)
        return india_count >= 2  # Requerir al menos 2 indicadores para mayor precisi√≥n

    async def setup_browser(self):
        """Configuraci√≥n de navegador SIMPLIFICADA que funciona"""
        self.playwright = await async_playwright().start()
        
        # Configuraci√≥n SIMPLE como el c√≥digo base que funciona
        self.browser = await self.playwright.chromium.launch(
            headless=False,
            args=['--start-maximized']  # Solo lo b√°sico
        )
        
        # Configuraci√≥n b√°sica sin scripts anti-detecci√≥n problem√°ticos
        self.context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            viewport={'width': 1366, 'height': 768}
        )
        
        self.page = await self.context.new_page()

    @ErrorHandler.async_retry(max_retries=3, delay=2, backoff=2)
    async def login_to_linkedin(self, email: str, password: str):
        try:
            self.metrics._log("üîê Iniciando sesi√≥n en LinkedIn...")
            await self.page.goto('https://www.linkedin.com/login', wait_until='networkidle')
            await asyncio.sleep(random.uniform(1, 3))
            
            # Verificar si ya estamos logueados
            try:
                await self.page.wait_for_selector('input[aria-label="Buscar"]', timeout=5000)
                self.metrics._log("‚úÖ Ya se encuentra logueado")
                return True
            except:
                pass  # Continuar con el login
            
            await self.page.fill('#username', email)
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
            await self.page.fill('#password', password)
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
            await self.page.click('button[type="submit"]')
            
            # Esperar con verificaci√≥n de login exitoso
            try:
                await self.page.wait_for_selector('input[aria-label="Buscar"]', timeout=15000)
                self.metrics._log("‚úÖ Sesi√≥n iniciada correctamente")
                return True
            except:
                # Verificar si hay error de login
                error_element = await self.page.query_selector('.alert-error')
                if error_element:
                    error_text = await error_element.inner_text()
                    raise Exception(f"Error en login: {error_text}")
                raise Exception("Timeout en login")
                
        except Exception as e:
            self.metrics._log(f"‚ùå Error en login: {e}", "ERROR")
            raise

    async def handle_popups(self):
        """Cerrar pop-ups emergentes - versi√≥n simplificada"""
        try:
            await asyncio.sleep(2)
            selectors = ['.artdeco-modal__dismiss', '.msg-overlay-bubble-header__control', 'button[aria-label="Dismiss"]']
            for selector in selectors:
                try:
                    close_btn = await self.page.query_selector(selector)
                    if close_btn and await close_btn.is_visible():
                        await close_btn.click()
                        await asyncio.sleep(1)
                except:
                    continue
        except Exception as e:
            self.metrics._log(f"‚ö†Ô∏è Error manejando pop-ups: {e}", "WARNING")

    def detect_language(self, text):
        """Detectar idioma del texto"""
        if text == "N/A" or len(text.strip()) < 10:
            return "unknown"
        
        try:
            lang, confidence = langid.classify(text)
            return lang
        except:
            return "unknown"

    def is_desired_language(self, text):
        """Verificar si el texto est√° en un idioma deseado"""
        if text == "N/A":
            return True
            
        undesired_languages = ['ar', 'hi', 'ur', 'bn', 'pa']
        
        lang = self.detect_language(text)
        
        if lang == "unknown":
            arabic_chars = bool(re.search('[\u0600-\u06FF]', text))
            hindi_chars = bool(re.search('[\u0900-\u097F]', text))
            
            if arabic_chars or hindi_chars:
                return False
            return True
        
        return lang not in undesired_languages

    async def verify_author_location(self, author_url, country):
        """Verificar la ubicaci√≥n del autor visitando su perfil - usa author_url SIN eliminarlo"""
        if author_url == "N/A":
            return False
            
        try:
            new_page = await self.context.new_page()
            await new_page.goto(author_url, wait_until='networkidle')
            await asyncio.sleep(2)
            
            location_selectors = [
                '.text-body-small.inline.t-black--light.break-words',
                '.pv-top-card--list-bullet li',
                '.pv-top-card-v2--list-bullet li'
            ]
            
            location = "N/A"
            for selector in location_selectors:
                location_element = await new_page.query_selector(selector)
                if location_element:
                    location = await location_element.inner_text()
                    if location and location.strip():
                        break
            
            await new_page.close()
            
            # Verificar primero si es India
            if self.is_india(location):
                return False
            
            # Indicadores para cada pa√≠s
            country_indicators = {
                "United States": ['usa', 'united states', 'estados unidos', 'us', 'u.s.', 'new york', 'california', 'texas'],
                "Canada": ['canada', 'canad√°', 'toronto', 'vancouver', 'montreal'],
                "United Kingdom": ['uk', 'united kingdom', 'london', 'manchester', 'england'],
                "Spain": ['spain', 'espa√±a', 'madrid', 'barcelona', 'valencia'],
                "Colombia": ['colombia', 'bogota', 'bogot√°', 'medellin', 'medell√≠n', 'cali', 'barranquilla', 'cartagena']
            }
            
            location_lower = location.lower()
            return any(indicator in location_lower for indicator in country_indicators.get(country, []))
            
        except Exception as e:
            self.metrics._log(f"‚ö†Ô∏è Error verificando ubicaci√≥n del autor: {e}", "WARNING")
            return False

    def verify_content_location(self, post_data, country):
        """Verificar ubicaci√≥n basada en el contenido del post"""
        text_content = f"{post_data['author_title']} {post_data['post_text']}".lower()
        
        # Verificar primero si es India
        if self.is_india(text_content):
            return False
        
        country_keywords = {
            "United States": ['usa', 'united states', 'estados unidos', 'us', 'u.s.', 'new york', 'california', 'texas'],
            "Canada": ['canada', 'canad√°', 'toronto', 'vancouver', 'montreal'],
            "United Kingdom": ['uk', 'united kingdom', 'london', 'manchester', 'england'],
            "Spain": ['spain', 'espa√±a', 'madrid', 'barcelona', 'valencia'],
            "Colombia": ['colombia', 'bogota', 'bogot√°', 'medellin', 'medell√≠n', 'cali', 'barranquilla', 'cartagena']
        }
        
        return any(keyword in text_content for keyword in country_keywords.get(country, []))

    async def go_to_next_page(self):
        """Navegar a la siguiente p√°gina - versi√≥n simplificada"""
        try:
            next_selectors = [
                'button[aria-label="Siguiente"]',
                'button[aria-label="Next"]',
                '.artdeco-pagination__button--next'
            ]
            
            for selector in next_selectors:
                next_button = await self.page.query_selector(selector)
                if next_button:
                    is_disabled = await next_button.get_attribute('disabled')
                    if not is_disabled:
                        await next_button.click()
                        await self.page.wait_for_load_state('networkidle')
                        await asyncio.sleep(random.uniform(3, 5))
                        return True
                    else:
                        return False
            
            return False
            
        except Exception as e:
            self.metrics._log(f"‚ö†Ô∏è Error navegando a siguiente p√°gina: {e}", "WARNING")
            return False

    async def process_search_term(self, search_term, country, country_info):
        """Procesa un t√©rmino de b√∫squeda - versi√≥n mejorada pero simplificada"""
        try:
            if len(self.results) >= self.max_posts_total:
                self.metrics._log(f"‚ö° L√≠mite global de {self.max_posts_total} publicaciones alcanzado")
                return False
                
            self.metrics._log(f"üîç Buscando publicaciones para: '{search_term}' en {country}")
            
            search_url = (
                f"https://www.linkedin.com/search/results/content/?"
                f"keywords={search_term.replace(' ', '%20')}"
                f"&facetGeoRegion={country_info['code']}"
                f"&facetContentType=%22posts%22"
            )
            
            await self.page.goto(search_url, wait_until='networkidle')
            await asyncio.sleep(random.uniform(3, 5))
            
            await self.handle_popups()
            
            pages_processed = 0
            
            while pages_processed < self.max_pages:
                if len(self.results) >= self.max_posts_total:
                    self.metrics._log(f"‚ö° L√≠mite global alcanzado")
                    return True
                    
                self.metrics._log(f"üìÑ Procesando p√°gina {pages_processed + 1}")
                
                try:
                    await self.page.wait_for_selector('.feed-shared-update-v2', timeout=10000)
                except:
                    self.metrics._log("‚ö†Ô∏è No se encontraron publicaciones")
                    break
                
                # Scroll simple
                for i in range(3):
                    await self.page.evaluate('''
                        window.scrollTo({
                            top: document.body.scrollHeight,
                            behavior: 'smooth'
                        })
                    ''')
                    await asyncio.sleep(random.uniform(2, 4))
                
                posts = await self.page.query_selector_all('.feed-shared-update-v2')
                self.metrics.increment('posts_found', len(posts))
                
                if not posts:
                    break
                    
                self.metrics._log(f"‚úÖ Encontradas {len(posts)} publicaciones. Procesando...")
                
                for i, post in enumerate(posts):
                    if len(self.results) >= self.max_posts_total:
                        return True
                        
                    current_term_count = len([r for r in self.results if r['search_term'] == search_term and r['country'] == country])
                    if current_term_count >= self.max_posts_per_term:
                        self.metrics._log(f"‚ö° L√≠mite por t√©rmino alcanzado para '{search_term}' en {country}")
                        return True
                    
                    try:
                        self.metrics._log(f"  üîç Procesando publicaci√≥n {i+1}/{len(posts)}")
                        post_data = await self.extract_post_data(post)
                        post_data['search_term'] = search_term
                        post_data['country'] = country
                        post_data['location'] = random.choice(country_info['location_names'])
                        
                        # Verificar si es de India
                        combined_text = f"{post_data['author_title']} {post_data['post_text']} {post_data['location']}"
                        if self.is_india(combined_text):
                            self.metrics._log(f"  üáÆüá≥ Publicaci√≥n de India detectada y filtrada")
                            self.metrics.increment('posts_filtered')
                            self.metrics.increment('india_posts_filtered')
                            continue
                        
                        if not self.is_desired_language(post_data['post_text']):
                            self.metrics._log(f"  ‚ö†Ô∏è Publicaci√≥n en idioma no deseado")
                            self.metrics.increment('posts_filtered')
                            continue
                        
                        location_verified = self.verify_content_location(post_data, country)
                        
                        if not location_verified and post_data.get('author_url', "N/A") != "N/A":
                            location_verified = await self.verify_author_location(post_data['author_url'], country)
                        
                        if location_verified:
                            self.results.append(post_data)  # MANTENER TODAS LAS URLs
                            self.metrics.increment('posts_added')
                            self.metrics.increment('countries_processed', 1, country)
                            self.metrics.increment('terms_processed', 1, search_term)
                            self.metrics._log(f"  ‚úÖ Publicaci√≥n a√±adida - Post: {post_data.get('post_url', 'N/A')}")
                        else:
                            self.metrics._log(f"  ‚ö†Ô∏è Publicaci√≥n fuera de {country}")
                            self.metrics.increment('posts_filtered')
                            
                    except Exception as e:
                        self.metrics._log(f"  ‚ö†Ô∏è Error en publicaci√≥n {i+1}: {e}", "WARNING")
                        self.metrics.increment('errors_count')
                
                # Intentar ir a la siguiente p√°gina
                if not await self.go_to_next_page():
                    self.metrics._log("‚úÖ No hay m√°s p√°ginas disponibles")
                    break
                
                pages_processed += 1
                
            return True
        except Exception as e:
            self.metrics._log(f"‚ùå Error procesando b√∫squeda '{search_term}' en {country}: {e}", "ERROR")
            self.metrics.increment('errors_count')
            return False

    async def search_and_extract_posts(self):
        """B√∫squeda principal - versi√≥n mejorada"""
        try:
            self.metrics._log(f"üîç Iniciando b√∫squeda con {len(self.search_terms)} t√©rminos en {len(self.countries)} pa√≠ses...")
            
            for country_name, country_info in self.countries.items():
                self.metrics._log(f"\n{'='*60}")
                self.metrics._log(f"üåé PA√çS: {country_name}")
                self.metrics._log(f"{'='*60}")
                
                for i, search_term in enumerate(self.search_terms):
                    if len(self.results) >= self.max_posts_total:
                        self.metrics._log(f"‚ö° L√≠mite global alcanzado")
                        return True
                        
                    self.metrics._log(f"\n  üîç T√©rmino {i+1}/{len(self.search_terms)}: '{search_term}'")
                    
                    success = await self.process_search_term(search_term, country_name, country_info)
                    
                    if not success:
                        self.metrics._log(f"  ‚ö†Ô∏è Error procesando t√©rmino: '{search_term}'")
                    
                    # Guardar cada 25 publicaciones
                    if len(self.results) % 25 == 0 and len(self.results) > 0:
                        await self.save_to_csv_and_excel()
                    
                    # Pausa entre t√©rminos
                    if i < len(self.search_terms) - 1:
                        pause_time = random.randint(5, 10)
                        self.metrics._log(f"  ‚è≥ Pausa de {pause_time} segundos...")
                        await asyncio.sleep(pause_time)
                
                # Pausa entre pa√≠ses
                if country_name != list(self.countries.keys())[-1]:
                    pause_time = random.randint(10, 15)
                    self.metrics._log(f"\nüåé Pausa de {pause_time} segundos antes del pr√≥ximo pa√≠s...")
                    await asyncio.sleep(pause_time)
            
            return len(self.results) > 0
        except Exception as e:
            self.metrics._log(f"‚ùå Error en b√∫squeda: {e}", "ERROR")
            self.metrics.increment('errors_count')
            return False

    def extract_url_from_data_urn(self, data_urn):
        """Extraer URL del post desde data-urn"""
        if data_urn and data_urn.startswith('urn:li:activity:'):
            activity_id = data_urn.split(':')[-1]
            return f"https://www.linkedin.com/feed/update/{activity_id}/"
        return None

    async def extract_post_data(self, post):
        """Extraer datos del post - COMPLETO con nombre, URLs y toda la informaci√≥n"""
        # Extraer NOMBRE del autor
        author_name = "N/A"
        try:
            author_element = await post.query_selector('.update-components-actor__name')
            author_name = await author_element.inner_text() if author_element else "N/A"
        except:
            pass
        
        # Extraer t√≠tulo del autor
        author_title = "N/A"
        try:
            title_element = await post.query_selector('.update-components-actor__description')
            author_title = await title_element.inner_text() if title_element else "N/A"
        except:
            pass
        
        # Extraer URL del PERFIL del autor
        author_url = "N/A"
        try:
            selectors = [
                'a.app-aware-link[href*="/in/"]',
                '.update-components-actor__container a',
                '.update-components-actor__name-link'
            ]
            
            for selector in selectors:
                link_element = await post.query_selector(selector)
                if link_element:
                    author_url = await link_element.get_attribute('href')
                    if author_url:
                        author_url = author_url.split('?')[0]
                        break
        except:
            pass
        
        # Extraer URL del POST
        post_url = "N/A"
        try:
            strategies = [
                ('a.app-aware-link[href*="/feed/update/"]', None),
                ('a[data-id*="share-"]', None),
                ('.update-components-actor__container a', lambda url: url if "/feed/update/" in url else None),
                ('.feed-shared-update-v2__content', None),
                ('.feed-shared-update-v2', self.extract_url_from_data_urn)
            ]
            
            for selector, processor in strategies:
                try:
                    element = await post.query_selector(selector)
                    if element:
                        if processor:
                            url = processor(await element.get_attribute('href') or await element.get_attribute('data-urn'))
                            if url:
                                post_url = url
                                break
                        else:
                            url = await element.get_attribute('href')
                            if url and "/feed/update/" in url:
                                post_url = url.split('?')[0]
                                break
                except:
                    continue
                    
        except Exception as e:
            self.metrics._log(f"‚ö†Ô∏è Error extrayendo URL de publicaci√≥n: {e}", "WARNING")
        
        # Extraer texto del post
        post_text = "N/A"
        try:
            text_element = await post.query_selector('.update-components-text')
            if text_element:
                post_text = await text_element.inner_text()
                post_text = post_text[:500]  # Limitar longitud
        except:
            pass
        
        # Extraer fecha del post
        post_date = "N/A"
        try:
            date_element = await post.query_selector('.update-components-actor__sub-description')
            if date_element:
                post_date = await date_element.inner_text()
                post_date = re.sub(r'‚Ä¢.*', '', post_date).strip()
        except:
            pass
        
        # Extraer ubicaci√≥n mostrada en el post
        location = "N/A"
        try:
            location_element = await post.query_selector('.update-components-actor__sub-description .update-components-actor__distance')
            if location_element:
                location = await location_element.inner_text()
        except:
            pass
        
        return {
            'author_name': author_name,        # RECUPERADO
            'author_title': author_title,
            'author_url': author_url,          # MANTENIDO - URL al perfil
            'post_url': post_url,              # RECUPERADO - URL al post
            'post_text': post_text,
            'post_date': post_date,
            'location': location,              # RECUPERADO
            'scraped_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    def generate_unique_id(self, row):
        """Genera un ID √∫nico para cada publicaci√≥n basado en m√∫ltiples campos"""
        # Si tenemos una URL v√°lida, usarla como identificador principal
        if row.get('post_url', 'N/A') != 'N/A' and pd.notna(row.get('post_url')):
            return f"url_{row['post_url']}"
        
        # Si no hay URL, usar combinaci√≥n de otros campos
        author_name = str(row.get('author_name', 'N/A'))
        post_text = str(row.get('post_text', 'N/A'))[:100]
        post_date = str(row.get('post_date', 'N/A'))
        
        # Crear un hash basado en la combinaci√≥n de campos
        combined_string = f"{author_name}_{post_text}_{post_date}"
        return f"hash_{hash(combined_string)}"

    def remove_duplicates_smart(self, df):
        """Elimina duplicados de manera inteligente MANTENIENDO concatenaci√≥n"""
        if len(df) == 0:
            return df
            
        df = df.copy()
        
        # Crear columna de ID √∫nico
        df['unique_id'] = df.apply(
            lambda row: self.generate_unique_id(row), 
            axis=1
        )
        
        # Eliminar duplicados manteniendo el √∫ltimo
        df = df.drop_duplicates(subset=['unique_id'], keep='last')
        
        # Eliminar la columna temporal
        df = df.drop(columns=['unique_id'])
        
        return df

    async def save_to_csv_and_excel(self):
        """CONCATENACI√ìN CORREGIDA - Funciona como el c√≥digo original"""
        if not self.results:
            self.metrics._log("‚ùå No hay datos para guardar")
            return
            
        new_df = pd.DataFrame(self.results)
        
        # Reordenar columnas INCLUYENDO todas las URLs
        column_order = ['search_term', 'country', 'location', 'author_name', 'author_title', 
                       'author_url', 'post_url', 'post_text', 'post_date', 'scraped_date']
        new_df = new_df.reindex(columns=column_order)
        
        # Archivo individual (como respaldo)
        individual_csv = f"linkedin_bim_posts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        new_df.to_csv(individual_csv, index=False, encoding='utf-8')
        self.metrics._log(f"‚úÖ Archivo individual guardado en {individual_csv}")
        
        # CONCATENACI√ìN CORRECTA - Archivo consolidado CSV
        if os.path.exists(self.consolidated_csv):
            self.metrics._log("üìÇ Leyendo archivo consolidado existente...")
            existing_df = pd.read_csv(self.consolidated_csv)
            self.metrics._log(f"üìä Publicaciones existentes: {len(existing_df)}")
            
            # CONCATENAR nuevos datos con existentes
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            self.metrics._log(f"üìä Total despu√©s de concatenar: {len(combined_df)}")
            
            # Eliminar duplicados
            combined_df = self.remove_duplicates_smart(combined_df)
            self.metrics._log(f"üìä Total despu√©s de eliminar duplicados: {len(combined_df)}")
            
            # Guardar archivo concatenado
            combined_df.to_csv(self.consolidated_csv, index=False, encoding='utf-8')
            self.metrics._log(f"‚úÖ Archivo consolidado CSV ACTUALIZADO: {self.consolidated_csv}")
        else:
            # Crear nuevo archivo consolidado
            new_df.to_csv(self.consolidated_csv, index=False, encoding='utf-8')
            self.metrics._log(f"‚úÖ Nuevo archivo consolidado CSV creado: {self.consolidated_csv}")
        
        # CONCATENACI√ìN CORRECTA - Archivo consolidado Excel
        if os.path.exists(self.consolidated_excel):
            self.metrics._log("üìÇ Leyendo archivo Excel consolidado existente...")
            existing_df = pd.read_excel(self.consolidated_excel)
            
            # CONCATENAR nuevos datos con existentes
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            
            # Eliminar duplicados
            combined_df = self.remove_duplicates_smart(combined_df)
            
            # Guardar archivo concatenado
            combined_df.to_excel(self.consolidated_excel, index=False)
            self.metrics._log(f"‚úÖ Archivo consolidado Excel ACTUALIZADO: {self.consolidated_excel}")
        else:
            # Crear nuevo archivo consolidado
            new_df.to_excel(self.consolidated_excel, index=False)
            self.metrics._log(f"‚úÖ Nuevo archivo consolidado Excel creado: {self.consolidated_excel}")
        
        # Reporte de estado
        self.metrics._log(f"üìä Publicaciones nuevas agregadas: {len(new_df)}")
        if os.path.exists(self.consolidated_csv):
            final_df = pd.read_csv(self.consolidated_csv)
            self.metrics._log(f"üìä TOTAL de publicaciones en archivo consolidado: {len(final_df)}")
            self.metrics._log(f"üíæ Archivo principal: {self.consolidated_csv}")
            self.metrics._log(f"üíæ Archivo Excel: {self.consolidated_excel}")

    async def run_scraper(self, email: str, password: str):
        """Ejecutar scraper - versi√≥n mejorada"""
        self.metrics.start_timer()
        
        max_attempts = 2  # Reducir intentos
        attempt = 0
        
        while attempt < max_attempts:
            try:
                attempt += 1
                self.metrics._log(f"üîÑ Intento {attempt}/{max_attempts}")
                
                await self.setup_browser()
                if await self.login_to_linkedin(email, password):
                    await self.handle_popups()
                    if await self.search_and_extract_posts():
                        await self.save_to_csv_and_excel()
                        break
                        
            except Exception as e:
                self.metrics.increment('errors_count')
                self.metrics._log(f"Error en intento {attempt}: {e}", "ERROR")
                
                if attempt >= max_attempts:
                    self.metrics._log("‚ùå Todos los intentos fallidos", "ERROR")
                else:
                    wait_time = 30 * attempt
                    self.metrics._log(f"‚è≥ Esperando {wait_time} segundos antes de reintentar...")
                    await asyncio.sleep(wait_time)
                    
            finally:
                try:
                    await self.browser.close()
                    await self.playwright.stop()
                    self.metrics._log("Navegador cerrado correctamente")
                except Exception as e:
                    self.metrics._log(f"Error cerrando navegador: {e}", "ERROR")
        
        self.metrics.end_timer()
        self.metrics.generate_report()

# ==================== EJECUCI√ìN PRINCIPAL ====================

async def main():
    print("=" * 80)
    print("üîç LINKEDIN BIM SCRAPER MEJORADO - VERSI√ìN H√çBRIDA")
    print("üîç T√©rminos de b√∫squeda: 15 t√©rminos BIM optimizados")
    print("üåé Pa√≠ses: 5 pa√≠ses principales")
    print("üìÑ Paginaci√≥n: 3 p√°ginas por t√©rmino")
    print("üìä L√≠mite: 500 publicaciones m√°ximas")
    print("üõ°Ô∏è Anti-scraping: Simplificado y funcional")
    print("üáÆüá≥ Filtro India: Activado - Detecta y filtra publicaciones de India")
    print("üìà M√©tricas: Activadas")
    print("=" * 80)
    
    scraper = LinkedInBIMScraperFixed()
    
    # CAMBIAR ESTAS CREDENCIALES POR LAS TUYAS
    email = "ussaapontejuandiego@gmail.com"
    password = "juandi778"
    
    if email == "tu_email@example.com":
        print("‚ùå Configura tus credenciales reales")
        return
    
    # Ejecutar el scraper
    await scraper.run_scraper(email, password)
    
    # Mensaje final
    print("=" * 80)
    print("üéâ PROCESO COMPLETADO")
    print(f"üìä Publicaciones obtenidas: {len(scraper.results)}")
    print(f"üáÆüá≥ Publicaciones de India filtradas: {scraper.metrics.metrics['india_posts_filtered']}")
    print("üíæ Datos guardados en archivos CSV y Excel")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(main())